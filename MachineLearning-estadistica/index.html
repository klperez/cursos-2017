<!DOCTYPE html>
<html>
<head>
  <title>Machine Learning</title>
  <meta charset="utf-8">
  <meta name="description" content="Machine Learning">
  <meta name="author" content="Kevin Pérez - Ing de Sistemas - Estadístico - (E) MSc. Ciencia de Datos">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="libraries/widgets/bootstrap/css/bootstrap.css"></link>
<link rel=stylesheet href="libraries/widgets/quiz/css/demo.css"></link>
<link rel=stylesheet href="libraries/widgets/interactive/css/aceeditor.css"></link>
<link rel=stylesheet href="libraries/widgets/nvd3/css/nv.d3.css"></link>
<link rel=stylesheet href="libraries/widgets/nvd3/css/rNVD3.css"></link>
<link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  <script src="libraries/widgets/nvd3/js/jquery-1.8.2.min.js"></script>
<script src="libraries/widgets/nvd3/js/d3.v3.min.js"></script>
<script src="libraries/widgets/nvd3/js/nv.d3.min-new.js"></script>
<script src="libraries/widgets/nvd3/js/fisheye.js"></script>


</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <aside class="gdbar">
    <img src="assets/img/unicordoba3.png">
  </aside>
  <hgroup class="auto-fadein">
    <h1>Machine Learning</h1>
    <h2>Programa de Estadística</h2>
    <p>Kevin Pérez - Ing de Sistemas - Estadístico - (E) MSc. Ciencia de Datos<br/>Departamento de Matemáticas y Estadística - Universidad de Córdoba</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Contenido programático</h2>
  </hgroup>
  <article data-timings="">
    <p><strong>Unidad de aprendizaje Nº 1.</strong> Generalidades </p>

<ul class = "build incremental">
<li>Conceptos Básicos del <strong><em>Machine Learning</em></strong>.</li>
<li>Diseño de un estudio de predicción. </li>
<li>Importancia relativa. </li>
<li>Error en y fuera de la muestra. </li>
<li>Tipos de errores. </li>
<li>Validación cruzada y tecnicas de remuestreo. </li>
<li>Preprocesamiento de los datos. </li>
<li>Medidas de calidad de los modelos </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Contenido programático</h2>
  </hgroup>
  <article data-timings="">
    <p><strong>Unidad de aprendizaje Nº 2.</strong>  <em>Machine Learning Supervisado</em></p>

<ul class = "build incremental">
<li><p>Modelos de regresión </p>

<ul>
<li>Regresión lineal simple y multiple </li>
</ul></li>
<li><p>Modelos de clasificación </p>

<ul>
<li>Análisis discriminante lineal </li>
<li>Regresión logistica </li>
</ul></li>
<li><p>Métodos basados en Árboles </p>

<ul>
<li>Árboles de decisión</li>
<li>Bagging</li>
<li>Random Forest </li>
<li>Boosting</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Contenido programático</h2>
  </hgroup>
  <article data-timings="">
    <p><strong>Unidad de aprendizaje Nº 3.</strong>  <em>Machine Learning No Supervisado</em></p>

<ul class = "build incremental">
<li><p>Métodos de reducción de dimensionalidad </p>

<ul>
<li>ACP, ACM, DVS</li>
</ul></li>
<li><p>Métodos Cluster </p>

<ul>
<li>K-Means </li>
<li>Cluster Jerárquico<br></li>
</ul></li>
<li><p>Reglas de asociación </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Contenido programático</h2>
  </hgroup>
  <article data-timings="">
    <p><strong>Unidad de aprendizaje Nº 4.</strong>  Otros métodos en <em>Machine Learning</em></p>

<ul class = "build incremental">
<li><p><em>Optimización: Algoritmos Geneticos</em></p></li>
<li><p><em>Support Vector Machines</em></p></li>
<li><p><em>Neural Networks</em></p></li>
<li><p><em>Pronosticos: Series de tiempo</em></p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Contenido programático</h2>
  </hgroup>
  <article data-timings="">
    <p><strong>Unidad de aprendizaje Nº 5.</strong>  Optimización de los modelos </p>

<ul class = "build incremental">
<li><p><em>Tunning</em></p></li>
<li><p><em>Regulización en regresión</em></p></li>
<li><p><em>Combinación de modelos</em></p></li>
<li><p><em>Predicción basada en modelos</em></p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Referencias</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Trevor H, Robert T, Jerome F,  <em>The Elements of Statistical Learning</em>, 2ª Edición, Springer.</p></li>
<li><p>Gareth J, Daniela W, Trevor H, Robert T, <em>An Introduction to Statistical Learning with Applications in R</em>, 6ª Edición, Springer. </p></li>
<li><p>Ethem A, <em>Introduction to Machine Learning</em>, 2ª Edición, The MIT Press
Cambridge, Massachusetts.</p></li>
<li><p>Max Kuhn, et all., <em>The caret package</em>, R CRAN, disponible en <a href="http://topepo.github.io/caret/index.html">http://topepo.github.io/caret/index.html</a>. </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Motivación</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Reconocido a nivel mundial en la academia</p>

<ul>
<li><a href="https://www.ualberta.ca/computing-science/graduate-studies/programs-and-admissions/statistical-machine-learning">Phd. in Machine Learning University of Alberta</a></li>
<li><a href="http://www.ml.cmu.edu/prospective-students/ml-phd.html">Phd. in Machine Learning Carnegie Mellon University</a></li>
</ul></li>
<li><p>Alta demanda laboral </p>

<ul>
<li><a href="http://hagutierrezro.blogspot.com.co/p/jobs.html">Statistical Jobs</a></li>
<li><a href="https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century">Data Scientist</a></li>
</ul></li>
<li><p>Un deporte moderno </p>

<ul>
<li><a href="https://www.kaggle.com/competitions">Competencias</a></li>
<li><a href="http://www.netflixprize.com">Premios</a></li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Que es <em>Machine Learning</em></h2>
  </hgroup>
  <article data-timings="">
    <ul class = "build incremental">
<li><p>Construcción/uso de algoritmos que <em><strong>aprenden</strong></em> de los datos </p></li>
<li><p>Más información implica mejor <em><strong>desempeño</strong></em></p></li>
<li><p>Soluciones previas implican <em><strong>Experiencia</strong></em> </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Ejemplo</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Etiquetar un cuadrado: Tamaño y borde  ---- Color </p></li>
<li><p>Previas observaciones (Etiquetadas por personas): </p></li>
</ul>

<p><center><img src="assets/img/img1.png" alt=""></center></p>

<ul>
<li><p>Tarea de la maquina: <em><strong>Etiquetar</strong></em> un nuevo cuadrado </p></li>
<li><p>Resultado: Éxito o Fracaso! </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Formulación</h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img2.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Que no es <em>Machine Learning</em></h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p><em><strong>No</strong></em> es machine learning </p>

<ul>
<li>Determinar el color que se presenta con mayor frecuencia </li>
<li>Calcular el tamaño promedio del cuadrado </li>
</ul></li>
<li><p><em><strong>La Meta principal</strong></em>: Construir modelos para la predicción  </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Un problema de  Regresión</h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img3.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Predicción</h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img4.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2><em>Statistical Learning</em></h2>
  </hgroup>
  <article data-timings="">
    <p>El <em>Statistical Learning</em> se refiere al vasto conjunto de herramientas para la <em>comprensión de los datos</em>. Estas herramientas pueden ser clasificadas <em>supervisadas</em> o <em>no supervisadas</em>.</p>

<ul class = "build incremental">
<li><p><em>supervised statistical learning:</em> Implica la construcción de un modelo estadístico para predecir, o estimar, una <em>salida</em> basada en una o más <em>entradas</em>.</p></li>
<li><p><em>unsupervised statistical learning:</em> Con estos modelos, existen <em>entradas</em> pero no existen salidas supervisadas, sin embargo se puede aprender de la estructura de los datos.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Por que estimar \(f\)</h2>
  </hgroup>
  <article data-timings="">
    <p>Existen dos razones principales por las cuales quisiéramos estimar \(f\): <em><strong>Predicción</strong></em> e <em><strong>Inferencia</strong></em>, teniendo en cuenta que: </p>

<p>\[\mathbf{Y}= f(\mathbf{Y})+ \epsilon\]</p>

<p>Asumiendo las restricciones de cada modelo y teniendo en cuenta la naturaleza de cada unas las variables involucradas en el mismo.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Predicción</h2>
  </hgroup>
  <article data-timings="">
    <p>En muchas situaciones, un conjunto de <em>entradas</em> \(X\) se encuentran disponibles, pero la salida \(Y\) no puede ser obtenida fácilmente. En esta situación y asumiendo que el termino de error es cero, podemos predecir \(Y\) utilizando </p>

<p>\[\hat{Y}=\hat{f}(X),\]
Donde \(\hat{f}\) representa nuestra estimación para \(f\) y \(\hat{Y}\) representa la predicción resultante para \(Y\). </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Predicción</h2>
  </hgroup>
  <article data-timings="">
    <p>Bajo estas condiciones \(\hat{f}\) a menudo es considerada una <em><strong>caja negra</strong></em> y en este sentido no se tiene en cuenta o nos interesa la forma de la función \(\hat{f}\) siempre que de ella resulten buenas predicciones. </p>

<p><center><img src="assets/img/img5.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Inferencia</h2>
  </hgroup>
  <article data-timings="">
    <p>En este caso a menudo el interés se centra no solo en una buena predicción, también en la forma en que \(Y\) se ve afectada por los cambios en \(X_1, \ldots, X_p\). En otras palabras estamos interesados en la relación que guarden \(X\) y \(Y\) o en los cambios de \(Y\) como función de \(X_1, \ldots, X_p\). En este sentido es lógico tratar de responder las siguientes preguntas</p>

<ul class = "build incremental">
<li>¿Qué predictores están asociados con la respuesta?</li>
<li>¿Cuál es la relación entre la respuesta y cada predictor?</li>
<li>¿Se puede resumir adecuadamente la relación entre \(Y\) y cada predictor, 
usando una ecuación lineal, o es la relación más complicada?</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Como estimamos \(f\)</h2>
  </hgroup>
  <article data-timings="">
    <p>Hablando de una manera muy general y teniendo en cuenta que queremos encontrar una función \(\hat{f}\) tal que \(Y\approx \hat{f}(X)\) para cada observación \((X, Y)\), los métodos estadísticos para esta tarea pueden ser clasificados como: </p>

<ul>
<li>Métodos Paramétricos </li>
<li>Métodos No Paramétricos </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Métodos Paramétricos</h2>
  </hgroup>
  <article data-timings="">
    <p>Los métodos paramétricos involucran un enfoque de dos pasos para el planteamiento del modelo</p>

<ol>
<li>Se asumen unos supuestos acerca de la forma funcional de \(f\), por ejemplo, un supuesto muy simple es que \(f\) es lineal en \(X\):</li>
</ol>

<p>\[f(X)= \beta_0+\beta_1X_1+\beta_2X_2+\cdots +\beta_pX_p\]
2. Una vez el modelo fue seleccionado, necesitamos un procedimiento de ajuste. En el caso del modelo lineal necesitamos estimar los parámetros \(\beta_0, \beta_1, \ldots, \beta_p\), esto es, encontrar los valores de esos parámetros tal que </p>

<p>\[Y\approx \beta_0+\beta_1X_1+\beta_2X_2+\cdots +\beta_pX_p\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Métodos Paramétricos</h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img6.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Métodos No Paramétricos</h2>
  </hgroup>
  <article data-timings="">
    <p>Los métodos no paramétricos hacen o lanzan supuestos explícitos acerca de la forma funcional de \(f\), en lugar de eso buscan una estimación de \(f\) que se aproxima a los puntos de los datos como sea posible sin ser demasiado áspera o sinuosa (ondulada).</p>

<p><center><img src="assets/img/img5.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Compensación entre flexibilidad e interpretabilidad</h2>
  </hgroup>
  <article data-timings="">
    <p>Los métodos estadísticos del <em>machine learning</em> propuestos anteriormente algunos son menos flexibles o menos restrictivos, en el sentido de que pueden producir sólo una gama relativamente pequeña de formas para estimar \(f\), otros métodos como el <em>thin plate splines</em> son mucho más flexibles porque pueden generar una gama mucho más amplia de formas posibles para estimar \(f\). </p>

<p><center><img src="assets/img/img7.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Componentes de un predictor</h2>
  </hgroup>
  <article data-timings="">
    <p><br></p>

<p><center> pregunta -&gt; data de entrada -&gt; características -&gt;</center></p>

<p><center> algoritmos -&gt; parámetros -&gt; evaluación </center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Ejemplo SPAM</h2>
  </hgroup>
  <article data-timings="">
    <p></br></p>

<p><center> <code>pregunta</code> -&gt; data de entrada -&gt; características -&gt;</center></p>

<p><center> algoritmos -&gt; parámetros -&gt; evaluación </center></p>

<p></br></p>

<p><strong>Comienza con una pregunta muy general</strong></p>

<p>Puedo detectar automáticamente los emails que son SPAM de los que no ?</p>

<p><strong>Más especifica</strong></p>

<p>Puedo utilizar características cuantitativas para clasificar los emails como SPAM/HAM</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Ejemplo SPAM</h2>
  </hgroup>
  <article data-timings="">
    <p></br></p>

<p><center> pregunta -&gt; <code>data de entrada</code> -&gt; características -&gt;</center></p>

<p><center> algoritmos -&gt; parámetros -&gt; evaluación </center></p>

<p></br></p>

<p>Véase <code>help(spam)</code> en el paquete <code>kernlab</code></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Ejemplo SPAM</h2>
  </hgroup>
  <article data-timings="">
    <p></br></p>

<p><center> pregunta -&gt; data de entrada -&gt; <code>características</code> -&gt;</center></p>

<p><center> algoritmos -&gt; parámetros -&gt; evaluación </center></p>

<p></br></p>

<p><b>
Dear Jeff, </p>

<p>Can you send me your address so I can send you the invitation? </p>

<p>Thanks,</p>

<p>Ben
</b></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Ejemplo SPAM</h2>
  </hgroup>
  <article data-timings="">
    <p></br></p>

<p><center> pregunta -&gt; data de entrada -&gt; <code>características</code> -&gt;</center></p>

<p><center> algoritmos -&gt; parámetros -&gt; evaluación </center>
</br></p>

<p><b> </p>

<p>Dear Jeff, </p>

<p>Can <rt>you</rt> send me your address so I can send <rt>you</rt> the invitation? </p>

<p>Thanks,</p>

<p>Ben
</b></p>

<p></br></p>

<p>Frecuencia de <code>you</code> \(= 2/17 = 0.118\)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Ejemplo SPAM</h2>
  </hgroup>
  <article data-timings="">
    <p></br></p>

<p><center> pregunta -&gt; data de entrada -&gt; <code>características</code> -&gt;</center></p>

<p><center> algoritmos -&gt; parámetros -&gt; evaluación </center>
</br></p>

<pre><code class="r">suppressMessages(suppressWarnings(library(kernlab))) 
data(spam)
head(spam, 3)[, 1:11]
</code></pre>

<pre><code>##   make address  all num3d  our over remove internet order mail receive
## 1 0.00    0.64 0.64     0 0.32 0.00   0.00     0.00  0.00 0.00    0.00
## 2 0.21    0.28 0.50     0 0.14 0.28   0.21     0.07  0.00 0.94    0.21
## 3 0.06    0.00 0.71     0 1.23 0.19   0.19     0.12  0.64 0.25    0.38
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Ejemplo SPAM</h2>
  </hgroup>
  <article data-timings="">
    <p><center> pregunta -&gt; data de entrada -&gt; características -&gt;</center></p>

<p><center> <code>algoritmos</code> -&gt; parámetros -&gt; evaluación </center></p>

<pre><code class="r">plot(density(spam$your[spam$type == &quot;nonspam&quot;]),
     col = &quot;blue&quot;, main = &quot;&quot;, xlab = &quot;Frecuencia de &#39;your&#39;&quot;, 
     frame.plot = F)
lines(density(spam$your[spam$type == &quot;spam&quot;]), col = &quot;red&quot;)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-2-1.png" title="plot of chunk unnamed-chunk-2" alt="plot of chunk unnamed-chunk-2" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>Ejemplo SPAM</h2>
  </hgroup>
  <article data-timings="">
    <p><center> pregunta -&gt; data de entrada -&gt; características -&gt;</center></p>

<p><center> <code>algoritmos</code> -&gt; parámetros -&gt; evaluación </center></p>

<p></br></br></p>

<p><strong>Nuestro algoritmo</strong></p>

<ul>
<li>Encuentre un valor \(C\). </li>
<li><strong>frecuencia de &#39;your&#39; \(>\) C</strong> prediga &quot;spam&quot;</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>Ejemplo SPAM</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">plot(density(spam$your[spam$type == &quot;nonspam&quot;]),
     col = &quot;blue&quot;, main = &quot;&quot;, xlab = &quot;Frecuencia de &#39;your&#39;&quot;, 
     frame.plot = F)
lines(density(spam$your[spam$type == &quot;spam&quot;]), col = &quot;red&quot;)
abline(v = 0.5,col = &quot;black&quot;)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-3-1.png" title="plot of chunk unnamed-chunk-3" alt="plot of chunk unnamed-chunk-3" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>Ejemplo SPAM</h2>
  </hgroup>
  <article data-timings="">
    <p><center> pregunta -&gt; data de entrada -&gt; características -&gt;</center></p>

<p><center>algoritmos-&gt; parámetros -&gt; <code>evaluación</code> </center></p>

<pre><code class="r">prediction &lt;- ifelse(spam$your &gt; 0.5, &quot;spam&quot;, &quot;nonspam&quot;)
table(prediction,spam$type)/length(spam$type)
</code></pre>

<pre><code>##           
## prediction   nonspam      spam
##    nonspam 0.4590306 0.1017170
##    spam    0.1469246 0.2923278
</code></pre>

<p></br>
<center>
Precisión $ \approx 0.459 + 0.292 = 0.751$
</center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>Orden de la importancia relativa</h2>
  </hgroup>
  <article data-timings="">
    <p><center> pregunta -&gt; data de entrada -&gt; características -&gt; algoritmos <center/></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>Basura entra = Basura sale</h2>
  </hgroup>
  <article data-timings="">
    <p><center> pregunta -&gt; <code>data de entrada</code> -&gt; características -&gt;</center></p>

<p><center> algoritmos -&gt; parámetros -&gt; evaluación </center></p>

<ol>
<li>Puede ser fácil (movie películas -&gt; nuevos ratings)</li>
<li>Puede ser difícil (data de genomas -&gt; enfermedades)</li>
<li>Si se puede (más data -&gt; mejores modelos)</li>
<li>Es el paso más importante ¡</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Características</h2>
  </hgroup>
  <article data-timings="">
    <p><center> pregunta -&gt; data de entrada -&gt; <code>características</code> -&gt;</center></p>

<p><center> algoritmos -&gt; parámetros -&gt; evaluación </center></p>

<p><strong>Propiedades de las buenas características</strong></p>

<ul>
<li>llevar a la comprensión de los datos </li>
<li>conservar la información relevante</li>
<li>son creadas con base a el conocimiento experto</li>
</ul>

<p><strong>Errores comunes</strong></p>

<ul>
<li>Tratar de automatizar la selección de características </li>
<li>No prestar atención a las particularidades de los datos </li>
<li>Despreciar información innecesariamente </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>Temas a considerar</h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img8.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>Compensación en predicción</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Interpretabilidad vs Precisión </p></li>
<li><p>Velocidad vs Precisión </p></li>
<li><p>Simplicidad vs Precisión </p></li>
<li><p>Escalabilidad vs Precisión</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>Error dentro vs Error fuera de la muestra</h2>
  </hgroup>
  <article data-timings="">
    <p><em><strong>In Sample Error</strong></em>: Es la tasa de error que se obtiene en el mismo conjunto de datos que utilizó para construir su predictor. A veces llamado error de resubstitución.</p>

<p><em><strong>Out of Sample Error</strong></em>: La tasa de error que obtiene en un nuevo conjunto de datos. A veces llamado error de generalización.</p>

<p><em><strong>Ideas principales</strong></em></p>

<ul>
<li><p><em><strong>Out of Sample Error</strong></em> es lo que nos preocupa </p></li>
<li><p>El <em><strong>In Sample Error</strong></em> \(<\) <em><strong>Out of Sample Error</strong></em></p></li>
<li><p>La razón principal es el sobre ajuste </p>

<ul>
<li>Acomodar su algoritmo con los datos que tiene</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>Error dentro vs Error fuera de la muestra</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">suppressMessages(suppressWarnings(library(kernlab)))
data(spam); set.seed(333)
smallSpam &lt;- spam[sample(dim(spam)[1],size = 10),]
spamLabel &lt;- (smallSpam$type == &quot;spam&quot;)*1 + 1
plot(smallSpam$capitalAve,col = spamLabel, frame.plot = FALSE, pch = 19)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-5-1.png" title="plot of chunk unnamed-chunk-5" alt="plot of chunk unnamed-chunk-5" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>Regla de predicción 1</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>capitalAve \(>\) 2.7 = &quot;spam&quot;</li>
<li>capitalAve \(<\) 2.40 = &quot;nonspam&quot;</li>
<li>capitalAve entre 2.40 y 2.45 = &quot;spam&quot;</li>
<li>capitalAve entre 2.45 y 2.7 = &quot;nonspam&quot;</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>Aplicando la regla 1 a <code>smallSpam</code></h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">rule1 &lt;- function(x){
  prediction &lt;- rep(NA,length(x))
  prediction[x &gt; 2.7] &lt;- &quot;spam&quot;
  prediction[x &lt; 2.40] &lt;- &quot;nonspam&quot;
  prediction[(x &gt;= 2.40 &amp; x &lt;= 2.45)] &lt;- &quot;spam&quot;
  prediction[(x &gt; 2.45 &amp; x &lt;= 2.70)] &lt;- &quot;nonspam&quot;
  return(prediction)
}
table(rule1(smallSpam$capitalAve),smallSpam$type)
</code></pre>

<pre><code>##          
##           nonspam spam
##   nonspam       5    0
##   spam          0    5
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Regla de predicción 2</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>capitalAve \(>\) 2.40 = &quot;spam&quot;</li>
<li>capitalAve \(\leq\) 2.40 = &quot;nonspam&quot;</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Apliclando la Regla 2 a <code>smallSpam</code></h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">rule2 &lt;- function(x){
  prediction &lt;- rep(NA,length(x))
  prediction[x &gt; 2.8] &lt;- &quot;spam&quot;
  prediction[x &lt;= 2.8] &lt;- &quot;nonspam&quot;
  return(prediction)
}
table(rule2(smallSpam$capitalAve),smallSpam$type)
</code></pre>

<pre><code>##          
##           nonspam spam
##   nonspam       5    1
##   spam          0    4
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Aplicando a la data completa <code>spam</code></h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">table(rule1(spam$capitalAve), spam$type)
</code></pre>

<pre><code>##          
##           nonspam spam
##   nonspam    2141  588
##   spam        647 1225
</code></pre>

<pre><code class="r">table(rule2(spam$capitalAve), spam$type)
</code></pre>

<pre><code>##          
##           nonspam spam
##   nonspam    2224  642
##   spam        564 1171
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Aplicando a la data completa <code>spam</code></h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">mean(rule1(spam$capitalAve) == spam$type)
</code></pre>

<pre><code>## [1] 0.7315801
</code></pre>

<pre><code class="r">mean(rule2(spam$capitalAve) == spam$type)
</code></pre>

<pre><code>## [1] 0.7378831
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>La precisión</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">sum(rule1(spam$capitalAve) == spam$type)
</code></pre>

<pre><code>## [1] 3366
</code></pre>

<pre><code class="r">sum(rule2(spam$capitalAve) == spam$type)
</code></pre>

<pre><code>## [1] 3395
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Que sucede?</h2>
  </hgroup>
  <article data-timings="">
    <p><center><code>Sobre ajuste</code></center></p>

<ul>
<li>La data tiene dos partes 

<ul>
<li>Patrones </li>
<li>Ruido</li>
</ul></li>
<li>La meta de una predicción es encontrar patrones </li>
<li>Siempre se puede diseñar un predictor perfecto en la muestra</li>
<li>Se capturan ambas patrones y ruidos cuando haces eso<br></li>
<li>El predictor no funcionará tan bien en nuevas muestras</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Diseño de un estudio de predicción</h2>
  </hgroup>
  <article data-timings="">
    <ol>
<li>Defina su tasa de error </li>
<li>Divida la data en:

<ul>
<li>Preparación, Prueba, Validación (opcional)</li>
</ul></li>
<li>En la data de preparación escoja las características (variables) 

<ul>
<li>Utilice <code>cross-validation</code></li>
</ul></li>
<li>En la data de preparación se escoja la función

<ul>
<li>Utilice <code>cross-validation</code></li>
</ul></li>
<li>Si no se hace validación 

<ul>
<li>Aplique 1x (Una vez) al conjunto de prueba</li>
</ul></li>
<li>Si se hace validación

<ul>
<li>Aplique al conjunto de prueba y refine </li>
<li>Aplique 1x (Una vez) al conjunto de validación </li>
</ul></li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Diseño de un estudio de predicción</h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img9.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>Reglas básicas para el diseño</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Si tiene un tamaño de muestra grande

<ul>
<li>60% Preparación</li>
<li>20% Prueba</li>
<li>20% Validación</li>
</ul></li>
<li>Si tiene un tamaño de muestra medio 

<ul>
<li>60% Preparación</li>
<li>40% Prueba</li>
</ul></li>
<li>Si tiene un tamaño de muestra pequeño

<ul>
<li>Haga <code>cross validation</code></li>
<li>Reporte la advertencia de tamaño de muestra pequeño </li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Terminos básicos</h2>
  </hgroup>
  <article data-timings="">
    <p>En general, <strong>Positivo</strong> = identificados y <strong>negativo</strong> = rechazado. Por lo tanto:</p>

<p><strong>Verdadero positivo</strong> = identificado correctamente </p>

<p><strong>Falso positivo</strong> = identificado incorrectamente </p>

<p><strong>Verdadero negativo</strong> = rechazado correctamente </p>

<p><strong>Falso negativo</strong> = rechazado incorrectamente </p>

<p><em>Ejemplo de un test Medico</em>:</p>

<p><strong>Verdadero positivo</strong> = Personas enfermas correctamente diagnosticadas como enfermas</p>

<p><strong>Falso positivo</strong>= Personas sanas identificadas incorrectamente como enfermas</p>

<p><strong>Verdadero negativo</strong> = Gente sana correctamente identificada como saludable</p>

<p><strong>Falso negativo</strong> = Personas enfermas identificadas incorrectamente como saludables.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Cantidades clave</h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img10.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Cantidades clave como fracciones</h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img11.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Para datos continuos</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><strong>Error cuadrático medio</strong></li>
</ul>

<p>\[\frac{1}{n}\sum_{i=1}^n (prediccion_i-verdadero_i)^2\]</p>

<ul>
<li><strong>Raíz error cuadrático medio</strong></li>
</ul>

<p>\[\sqrt{\frac{1}{n}\sum_{i=1}^n (prediccion_i-verdadero_i)^2}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>Medidas comunes de error</h2>
  </hgroup>
  <article data-timings="">
    <ol>
<li>Error cuadrático medio

<ul>
<li>Datos continuos, sensible a los `outliers</li>
</ul></li>
<li>Desviación media absoluta 

<ul>
<li>Datos continuos, A menudo más robusta</li>
</ul></li>
<li>Sensibilidad <em>Sensitivity</em> 

<ul>
<li>Si desea perder algunos positivos</li>
</ul></li>
<li>Especificidad <em>Specificity</em>

<ul>
<li>Si quieres algunos negativos llamados positivos</li>
</ul></li>
<li>Precisión <em>Accuracy</em>

<ul>
<li>Pondera falsos positivos/negativos igualmente</li>
</ul></li>
<li>Concordancia <em>Concordance</em>

<ul>
<li>Un ejemplo en <a href="http://en.wikipedia.org/wiki/Cohen%27s_kappa">kappa</a></li>
</ul></li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-57" style="background:;">
  <hgroup>
    <h2>Métodos de remuestreo</h2>
  </hgroup>
  <article data-timings="">
    <ul class = "build incremental">
<li><p>Los métodos de remuestreo son una herramienta indispensable en la estadística moderna. Ellos implican extraer repetidamente muestras de un conjunto de entrenamiento y volver a ajustar un modelo de interés en cada muestra con el fin de obtener información adicional sobre el modelo ajustado</p></li>
<li><p>Los enfoques de re-muestreo pueden ser computacionalmente caros, ya que implican ajustar el mismo método estadístico varias veces utilizando diferentes subconjuntos de los datos de entrenamiento. Sin embargo, debido a los recientes avances en el poder de cálculo, los requisitos computacionales de los métodos de remuestreo generalmente no son prohibitivos.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-58" style="background:;">
  <hgroup>
    <h2>Validación Cruzada - <em>(Cross Validation)</em></h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img9.png" alt=""></center></p>

<p><a href="http://www2.research.att.com/%7Evolinsky/papers/ASAStatComp.pdf">http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-59" style="background:;">
  <hgroup>
    <h2>Validación Cruzada - <em>(Cross Validation)</em></h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>La precisión en los datos de preparación es optimista </p></li>
<li><p>Estimaciones mas confiables provienen de conjuntos de datos independientes </p></li>
<li><p>Pero no podemos usar el conjunto de pruebas al construir el modelo o se convierte en parte del conjunto de entrenamiento.</p></li>
<li><p>Por lo tanto, estimamos la precisión del conjunto de pruebas con el conjunto de entrenamiento.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-60" style="background:;">
  <hgroup>
    <h2>Validación Cruzada - <em>(Cross Validation)</em></h2>
  </hgroup>
  <article data-timings="">
    <p><em><strong>Propuesta:</strong></em></p>

<ul>
<li>Utilizar el conjunto de datos de entrenamiento </li>
<li>Dividalo en <em>entrenamiento/prueba</em> </li>
<li>Construya el modelo en la data de <em>entrenamiento</em></li>
<li>Evalué el modelo en la data de <em>prueba</em></li>
<li>Repita y promedie los errores estimados </li>
</ul>

<p><em><strong>Se utiliza:</strong></em></p>

<ul>
<li>Escoger las variables a incluir en el modelo </li>
<li>Escoger el tipo de función de predicción a utilizar </li>
<li>Escoger los parámetros en la función de predicción </li>
<li>Comparar diferentes predictores</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-61" style="background:;">
  <hgroup>
    <h2>Conjunto de validación</h2>
  </hgroup>
  <article data-timings="">
    <p>Supongamos que nos gustaría estimar el error de prueba asociado con el ajuste de un método de aprendizaje estadístico particular en un conjunto de observaciones. El enfoque de <em>Conjunto de validación</em> </p>

<p>La tasa de error de conjunto de validación resultante, normalmente se evalúa utilizando \(ECM\) en el caso de una respuesta cuantitativa proporciona una estimación de la tasa de error de prueba.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-62" style="background:;">
  <hgroup>
    <h2>Conjunto de validación</h2>
  </hgroup>
  <article data-timings="">
    <p>El enfoque del conjunto de validaciones es conceptualmente simple y fácil de implementar. Pero tiene dos inconvenientes potenciales:</p>

<ol>
<li><p>La estimación de validación de la tasa de error de prueba puede ser muy variable, dependiendo de qué observaciones se incluyen en el conjunto de entrenamiento y qué observaciones se incluyen en el conjunto de validación.</p></li>
<li><p>En el enfoque de validación, sólo un subconjunto de las observaciones que se incluyen en el conjunto de entrenamiento en lugar de en el conjunto de validación se utilizan para adaptarse al modelo. Dado que los métodos estadísticos tienden a rendir peor cuando se entrenan en menos observaciones, esto sugiere que la tasa de error de validación establecido puede tender a sobrestimar la tasa de error de prueba para el ajuste del modelo en todo el conjunto de datos.</p></li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-63" style="background:;">
  <hgroup>
    <h2><em>Leave-One-Out Cross-Validation</em></h2>
  </hgroup>
  <article data-timings="">
    <p><em>Leave-one-out cross-validation (LOOCV)</em> está muy relacionada con el el enfoque de conjunto de validación, como en este caso el proceso involucra dividir el conjunto de observaciones en dos partes, Sin embargo, en lugar de crear dos subconjuntos de tamaño comparable, una sola observación \((x_1, y_1)\) Se utiliza para el conjunto de validación, y las restantes observaciones \(\{(x_2, y_2) , \ldots, (x_n, y_n)\}\) componen el conjunto de entrenamiento.</p>

<p>La estimación <em>LOOCV</em> para la el conjunto de prueba  es el promedio de \(ECM\) de estas <em>n</em> estimaciones:</p>

<p>\[CV_{(n)}=\frac{1}{n}\sum_{i=1}^nECM_{i}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-64" style="background:;">
  <hgroup>
    <h2><em>Leave-One-Out Cross-Validation</em></h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img12.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-65" style="background:;">
  <hgroup>
    <h2><em>k-Fold Cross-Validation</em></h2>
  </hgroup>
  <article data-timings="">
    <p>Una alternativa al <em>LOOCV</em> es <em>k-fold CV</em>, este enfoque involucra dividir aleatoriamente el conjunto de observaciones en <em>k</em> grupos o <em>folds</em>, de aproximadamente el mismo tamaño. El primer <em>folds</em> se trata como un conjunto de prueba, y el modelo se ajusta en los restantes \(k-1\) <em>folds</em>.</p>

<p>La estimación del <em>k-fold CV</em>  se calcula promediando estos valores,</p>

<p>\[CV_{(k)}=\frac{1}{k}\sum_{i=1}^kECM_{i}\]</p>

<p>No es difícil observar que <em>LOOCV</em> es un caso especial de <em>k-fold CV</em> en el que \(k\) se fija a igual \(n\).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-66" style="background:;">
  <hgroup>
    <h2><em>Bootstrap</em></h2>
  </hgroup>
  <article data-timings="">
    <p>El <em>bootstrap</em> es una herramienta estadística ampliamente aplicable y extremadamente poderosa que se puede utilizar para cuantificar la incertidumbre asociada con un estimador dado o método de aprendizaje estadístico. Con este enfoque seleccionamos aleatoriamente \(n\) observaciones del conjunto de datos para producir un conjunto de datos bootstrap \(Z^{*1}\) para producir nuevas estimaciones para un \(\alpha\) que llamaremos \(\hat{\alpha}^{*1}\),(el muestreo se realiza con reemplazo).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-67" style="background:;">
  <hgroup>
    <h2><em>Bootstrap</em></h2>
  </hgroup>
  <article data-timings="">
    <p>Este procedimiento se repite \(B\) veces para algún valor grande \(B\), con el fin de producir \(B\) diferentes conjuntos de datos de arranque \(Z^{*1}, Z^{*2}, \ldots, Z^{*B}\) y las \(B\) correspondientes \(\alpha\) estimaciones \(\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, \ldots, \hat{\alpha}^{*B}\) Podemos calcular el error estándar de estas estimaciones de bootstrap usando la fórmula</p>

<p>\[SE_{B(\hat{\alpha})}=\sqrt{\frac{1}{B-1}\sum_{r=1}^B\left(\hat{\alpha}^{*r}-\frac{1}{B} \sum_{r'=1}^B \hat{\alpha}^{*r'}\right)}\] </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-68" style="background:;">
  <hgroup>
    <h2><em>Bootstrap</em></h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/img13.png" alt=""></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-69" style="background:;">
  <hgroup>
    <h2>Regresión Lineal</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Considere el desarrollo probabilistico para un modelo de regresión lineal </li>
</ul>

<p>\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_{i}
\]</p>

<ul>
<li>Aquí \(\epsilon_{i}\) se asume <em>iid</em> \(N(0, \sigma^2)\). </li>
<li>Note que, \(E[Y_i ~|~ X_i = x_i] = \mu_i = \beta_0 + \beta_1 x_i\)</li>
<li>Note que, \(Var(Y_i ~|~ X_i = x_i) = \sigma^2\).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-70" style="background:;">
  <hgroup>
    <h2>Verosimilitud</h2>
  </hgroup>
  <article data-timings="">
    <p>\[L(\beta, \sigma)=\prod_{i=1}^n\{(2\pi\sigma^2)^{-1/2}\exp(-\frac{1}{2\sigma^2}(y_i-\mu_i)^2)\}\]
Entonces el doble del logaritmo negativo de la verosimilitud </p>

<p>\[-2\log\{L(\beta, \sigma)\}= \frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\mu_i)^2+n\log(\sigma^2)\]
El estimador de mínimos cuadrados para \(\mu = \beta_0 + \beta_1X_i\) es exactamente el estimador máximo verosimilitud. </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-71" style="background:;">
  <hgroup>
    <h2>Regresión Lineal</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>El modelo \(Y_i =  \mu_i + \epsilon_i = \beta_0 + \beta_1 X_i + \epsilon_i\) donde \(\epsilon_i\) están <em>iid</em> \(N(0, \sigma^2)\)</li>
<li>Los estimadores <em>MV</em> \(\beta_0\) y \(\beta_1\) son las estimaciones de mínimos cuadrados
\[\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X\]</li>
<li>\(E[Y ~|~ X = x] = \beta_0 + \beta_1 x\)</li>
<li>\(Var(Y ~|~ X = x) = \sigma^2\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-72" style="background:;">
  <hgroup>
    <h2>Interpretando los coeficientes</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>\(\beta_0\) es el valor esperado de la respuesta cuando el predictor es 0</li>
</ul>

<p>\[
E[Y | X = 0] =  \beta_0 + \beta_1 \times 0 = \beta_0
\]</p>

<ul>
<li><p>Tenga en cuenta que esto no siempre es de interés, por ejemplo cuando \(X=0\) en mediciones de presión sanguínea o la altura de una persona </p></li>
<li><p>Considere lo siguiente</p></li>
</ul>

<p>\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
= \beta_0 + a \beta_1 + \beta_1 (X_i - a) + \epsilon_i
= \tilde \beta_0 + \beta_1 (X_i - a) + \epsilon_i
\]</p>

<p>Entonces, cambiando los valores \(X\) por valores \(a\) cambia el intercepto, pero no la pendiente. </p>

<ul>
<li>A menudo \(a\) se establece como \(\bar X\) de modo que el intercepto se interpreta como la respuesta esperada en la media de los valores \(X\).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-73" style="background:;">
  <hgroup>
    <h2>Interpretando los coeficientes</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>\(\beta_1\) es el cambio esperado en la respuesta para un cambio de 1 unidad en el predictor
\[
E[Y ~|~ X = x+1] - E[Y ~|~ X = x] =
\beta_0 + \beta_1 (x + 1) - (\beta_0 + \beta_1 x ) = \beta_1
\]</li>
<li>Considere el impacto de cambiar las unidades de \(X\).</li>
</ul>

<p>\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
= \beta_0 + \frac{\beta_1}{a} (X_i a) + \epsilon_i
= \beta_0 + \tilde \beta_1 (X_i a) + \epsilon_i
\]</p>

<ul>
<li><p>Por lo tanto, la multiplicación de \(X\) por un factor \(a\) resulta en dividir el coeficiente por un factor de \(a\).</p></li>
<li><p>Ejemplo: \(X\) es la altura en \(m\) y \(Y\) es el peso en \(kg\). Entonces \(\beta_1\) es \(kg/m\). Convertir \(X\) a \(cm\) implica multiplicar \(X\) por \(100 cm/m\). Para obtener \(\beta_1\) en las unidades correctas, tenemos que dividir por \(100cm/m\) para que tenga las unidades correctas.</p></li>
</ul>

<p>\[
X m \times \frac{100cm}{m} = (100 X) cm
~~\mbox{y}~~
\beta_1 \frac{kg}{m} \times\frac{1 m}{100cm} = 
\left(\frac{\beta_1}{100}\right)\frac{kg}{cm}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-74" style="background:;">
  <hgroup>
    <h2>Predicción</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Si queremos predecir el resultado en un valor particular del predictor, digamos \(X\), el modelo de regresión predice</li>
</ul>

<p>\[
\hat \beta_0 + \hat \beta_1 X
\]</p>

<ul>
<li>Note que a un valor observado \(X\), se le obtienen predicciones 
\[\hat{\mu}_i=\hat{Y}_i= \hat \beta_0 + \hat \beta_1 X\]</li>
<li>Recuerde que los mínimos cuadrados hacen </li>
</ul>

<p>\[\sum_{i=1}^n(Y_i-\mu_i)\]
Para \(\mu_i\) expresadas como puntos de una linea </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-75" style="background:;">
  <hgroup>
    <h2>Ejemplo</h2>
  </hgroup>
  <article data-timings="">
    <p>El conjunto de datos <code>diamond</code> del paquete <code>UsingR</code> nos muestra los precios del diamante (<em>dólares de Singapur</em>) y el peso del diamante en quilates (medida estándar de masa de diamante, 0.2 \(g\)). Para obtener el uso de los datos</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-76" style="background:;">
  <hgroup>
    <h2>Ejemplo</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="assets/fig/unnamed-chunk-11-1.png" title="plot of chunk unnamed-chunk-11" alt="plot of chunk unnamed-chunk-11" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-77" style="background:;">
  <hgroup>
    <h2>Ajustando la regresión lineal</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">fit &lt;- lm(price ~ carat, data = diamond)
coef(fit)
</code></pre>

<pre><code>## (Intercept)       carat 
##   -259.6259   3721.0249
</code></pre>

<ul>
<li><p>Estimamos un aumento esperado de 3721.02 (SIN) del precio en dolares por cada aumento de quilates en la masa de diamante.</p></li>
<li><p>El intercepto -259.63 es el precio esperado de un diamante de 0 quilates.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-78" style="background:;">
  <hgroup>
    <h2>\(\hat \beta_0\) Interpretable</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">fit2 &lt;- lm(price ~ I(carat - mean(carat)), data = diamond)
coef(fit2)
</code></pre>

<pre><code>##            (Intercept) I(carat - mean(carat)) 
##               500.0833              3721.0249
</code></pre>

<p>Esto es, $500.1 es el precio esperado para 
el diamante de tamaño promedio de los datos (0.2041667 carats).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-79" style="background:;">
  <hgroup>
    <h2>Cambio de escala</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Un aumento de un quilate en un diamante es bastante, ¿qué pasa con un cambio de unidades de 1/10 de un quilate?</li>
<li>Sólo podemos hacer esto dividiendo el coeficiente por 10.</li>
<li>Esperamos un aumento 372.102 (SIN) del precio en dolares por cada 1/10th de aumento de quilates en masa de diamante.</li>
</ul>

<pre><code class="r">fit3 &lt;- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
</code></pre>

<pre><code>##   (Intercept) I(carat * 10) 
##     -259.6259      372.1025
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-80" style="background:;">
  <hgroup>
    <h2>Predicciones del precio de un diamante</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">newx &lt;- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx
</code></pre>

<pre><code>## [1]  335.7381  745.0508 1005.5225
</code></pre>

<pre><code class="r">predict(fit, newdata = data.frame(carat = newx))
</code></pre>

<pre><code>##         1         2         3 
##  335.7381  745.0508 1005.5225
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-81" style="background:;">
  <hgroup>
    <h2>Regresión multivariada</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>El modelo lineal general extiende la regresión lineal simple mediante la adición de términos de forma lineal en el modelo.</li>
</ul>

<p>\[
Y_i =  \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots +
\beta_{p} X_{pi} + \epsilon_{i} 
= \sum_{k=1}^p X_{ik} \beta_j + \epsilon_{i}
\]</p>

<ul>
<li>Aquí normalmente \(X_{1i}=1\), así que el termino para intercepto se incluye.</li>
<li><p>Los mínimos cuadrados (y por lo tanto las estimaciones <em>MV</em> bajo <em>iid</em> Normal de los errores) minimiza
\[
\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2
\]</p></li>
<li><p>Notese que la linealidad importante, linealidad en los coeficientes. Esto es,</p></li>
</ul>

<p>\[
Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots +
\beta_{p} X_{pi}^2 + \epsilon_{i} 
\]
Sigue siendo un modelo lineal. </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-82" style="background:;">
  <hgroup>
    <h2>Estimaciones</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Recordemos que la estimación <em>MC</em> para la regresión a través del origen,
\(E[Y_i]=X_{1i}\beta_1\), era \(\sum X_i Y_i / \sum X_i^2\)</p></li>
<li><p>Consideremos dos regresores, \(E[Y_i] = X_{1i}\beta_1 + X_{2i}\beta_2 = \mu_i\). </p></li>
<li><p>Mínimos cuadrados trata de minimizar </p></li>
</ul>

<p>\[
\sum_{i=1}^n (Y_i - X_{1i} \beta_1 - X_{2i} \beta_2)^2
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-83" style="background:;">
  <hgroup>
    <h2>Las ecuaciones</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Las soluciones de mínimos cuadrados tienen que minimizar
\[
\sum_{i=1}^n (Y_i - X_{1i}\beta_1 - \ldots - X_{pi}\beta_p)^2
\]</p></li>
<li><p>La estimación de mínimos cuadrados para el coeficiente de un modelo de regresión multivariada es exactamente la regresión a través del origen eliminado las relaciones lineales con los otros regresores tanto del regresor como de la salida tomando residuos.</p></li>
<li><p>En este sentido, la regresión multivariada &quot;ajusta&quot; un coeficiente del impacto lineal de las otras variables.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-84" style="background:;">
  <hgroup>
    <h2>Ejemplo</h2>
  </hgroup>
  <article data-timings="">
    <p>Modelo lineal con dos variable </p>

<pre><code class="r">n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n)
y = 1 + x + x2 + x3 + rnorm(n, sd = .1)
ey = resid(lm(y ~ x2 + x3))
ex = resid(lm(x ~ x2 + x3))
sum(ey * ex) / sum(ex ^ 2)
</code></pre>

<pre><code>## [1] 1.016763
</code></pre>

<pre><code class="r">coef(lm(ey ~ ex - 1))
</code></pre>

<pre><code>##       ex 
## 1.016763
</code></pre>

<pre><code class="r">coef(lm(y ~ x + x2 + x3)) 
</code></pre>

<pre><code>## (Intercept)           x          x2          x3 
##    1.015085    1.016763    1.010533    1.010018
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-85" style="background:;">
  <hgroup>
    <h2>Interpretación de los coeficientes</h2>
  </hgroup>
  <article data-timings="">
    <p>\[E[Y | X_1 = x_1, \ldots, X_p = x_p] = \sum_{k=1}^p x_{k} \beta_k\]</p>

<p>\[
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p] = (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k
\]</p>

<p>\[
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p]  - E[Y | X_1 = x_1, \ldots, X_p = x_p]
\]</p>

<p>\[= (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k + \sum_{k=1}^p x_{k} \beta_k = \beta_1 
\] De manera que la interpretación de un coeficiente de regresión multivariada, es el cambio esperado en la respuesta por unidad de cambio en el regresor, manteniendo fijos todos los otros regresores.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-86" style="background:;">
  <hgroup>
    <h2>Valores ajustados, residuos y variación residual</h2>
  </hgroup>
  <article data-timings="">
    <p>Todos los cálculos de (<em>RLS</em>) pueden extenderse a modelos lineales</p>

<ul>
<li>Modelo \(Y_i = \sum_{k=1}^p X_{ik} \beta_{k} + \epsilon_{i}\) donde \(\epsilon_i \sim N(0, \sigma^2)\)</li>
<li>Respuesta ajustada \(\hat Y_i = \sum_{k=1}^p X_{ik} \hat \beta_{k}\)</li>
<li>Residuales \(e_i = Y_i - \hat Y_i\)</li>
<li>Varianza estimada \(\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i ^2\)</li>
<li>Obtener predicciones en nuevos valores, \(x_1, \ldots, x_p\), simplemente,  \(\sum_{k=1}^p x_{k} \hat \beta_{k}\)</li>
<li>Los coeficientes errores estándar, \(\hat \sigma_{\hat \beta_k}\), y
\(\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}\)
siguen una distribución \(T\) con \(n-p\) grados de libertad.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-87" style="background:;">
  <hgroup>
    <h2>Modelos lineales</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Son la técnica estadística y de <em>machine learning</em> más aplicada por mucho</p></li>
<li><p>Algunas cosas asombrosas que puedes lograr con modelos lineales</p>

<ul>
<li>Ajustar flexiblemente funciones complicadas.</li>
<li>Ajustar variables tipo factor como predictoras </li>
<li>Construir modelos precisos </li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-88" style="background:;">
  <hgroup>
    <h2>Modelos lineales</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Los modelos lineales son la técnica estadística aplicada más útil. Sin embargo, no vienen sin sus limitaciones.</p>

<ul>
<li>Los modelos de respuesta aditiva no tienen mucho sentido si la respuesta es discreta, o estrictamente positiva.</li>
<li>Los modelos de error aditivo a menudo no tienen sentido, por ejemplo si el resultado tiene que ser positivo.</li>
<li>Las transformaciones son a menudo difíciles de interpretar.</li>
<li>Hay que modelar valores en los datos en la escala que fue recolectada.</li>
<li>Las transformaciones particularmente interpetables, los logaritmos naturales en específico, no son aplicables para valores negativos o cero.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-89" style="background:;">
  <hgroup>
    <h2>Modelos lineales generalizados</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Introducido en un articulo de RSSB 1972 por Nelder y Wedderburn.</li>
<li><p>Incluyen tres componentes:</p>

<ul>
<li>Un modelo de la <em>familia exponencial</em> para la respuesta. </li>
<li>Un componente sistemático a través de un predictor lineal</li>
<li>Una función de enlace que conecta la media de la respuesta al predictor lineal</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-90" style="background:;">
  <hgroup>
    <h2>Ejemplo, Modelos lineales</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Asuma que \(Y_i \sim N(\mu_i, \sigma^2)\) (La distribución Gaussiana es de la familia exponencial.)</li>
<li>Define el predictor lineal \(\eta_i = \sum_{k=1}^p X_{ik} \beta_k\).</li>
<li><p>La función de enlace es \(g\) tal que \(g(\mu) = \eta\).</p>

<ul>
<li>Para modelos lineales es \(g(\mu) = \mu\) tal que \(\mu_i = \eta_i\)</li>
</ul></li>
<li><p>Esto produce el mismo modelo de verosimilitud que nuestro modelo aditivo de error lineal de Gauss</p></li>
</ul>

<p>\[Y_i = \sum_{k=1}^p X_{ik} \beta_k + \epsilon_{i}\]</p>

<p>Donde \(\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-91" style="background:;">
  <hgroup>
    <h2>Ejemplo, Regresión logistica</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Asuma que \(Y_i \sim Bernoulli(\mu_i)\) tal que \(E[Y_i] = \mu_i\) donde \(0\leq \mu_i \leq 1\).</li>
<li>El predictor lineal \(\eta_i = \sum_{k=1}^p X_{ik} \beta_k\)</li>
<li><p>Función de enlace \(g(\mu) = \eta = \log\left( \frac{\mu}{1 - \mu}\right)\) \(g\) el log (natural) de los <em>odds</em>, conocidos como los <strong>logit</strong>.</p></li>
<li><p>Note que que podemos invertir la función <em><em>logit</em></em> como</p></li>
</ul>

<p>\[
\mu_i = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} ~~~\mbox{y}~~~
1 - \mu_i = \frac{1}{1 + \exp(\eta_i)}
\]
Por lo tanto, la verosimilitud es
\[
\prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1-y_i} = \exp\left(\sum_{i=1}^n y_i \eta_i \right)
\prod_{i=1}^n (1 + \eta_i)^{-1}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-92" style="background:;">
  <hgroup>
    <h2>Ejemplo, Regresión Poisson</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Asuma  que  \(Y_i \sim Poisson(\mu_i)\) tal que \(E[Y_i] = \mu_i\) donde \(0\leq \mu_i\)</li>
<li>Predictor lineal \(\eta_i = \sum_{k=1}^p X_{ik} \beta_k\)</li>
<li>Función de enlace \(g(\mu) = \eta = \log(\mu)\)</li>
<li>Recuerde que \(e^x\) es la inversa de \(\log(x)\) así,<br>
\[
\mu_i = e^{\eta_i}
\]
Por lo tanto, la verosimilitud es</li>
</ul>

<p>\[
\prod_{i=1}^n (y_i !)^{-1} \mu_i^{y_i}e^{-\mu_i} \propto \exp\left(\sum_{i=1}^n y_i \eta_i - \sum_{i=1}^n \mu_i\right)
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-93" style="background:;">
  <hgroup>
    <h2>Detalles</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>En cada caso, la única manera en que la verosimilitud depende de los datos es a través de</li>
</ul>

<p>\[\sum_{i=1}^n y_i \eta_i = \sum_{i=1}^n y_i\sum_{k=1}^p X_{ik} \beta_k = \sum_{k=1}^p \beta_k\sum_{i=1}^n X_{ik} y_i \]</p>

<p>Así, si no necesitamos todos los datos completos, sólo \(\sum_{i=1}^n X_{ik} y_i\). </p>

<p>Esta simplificación es una consecuencia de la elección de las llamadas funciones de enlace &quot;canónicas&quot;.</p>

<ul>
<li>(Esto tiene que ser derivado). Todos los modelos alcanzan su máximo en la raíz de las llamadas ecuaciones normales</li>
</ul>

<p>\[ 0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{Var(Y_i)}W_i \]
Donde \(W_i\) son la derivada de la inversa de la función de enlace.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-94" style="background:;">
  <hgroup>
    <h2>Acerca de las varianzas</h2>
  </hgroup>
  <article data-timings="">
    <p>\[
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{Var(Y_i)}W_i
\]</p>

<ul>
<li>Para el modelo lineal \(Var(Y_i) = \sigma^2\) es constante.</li>
<li>Para el caso Bernoulli \(Var(Y_i) = \mu_i (1 - \mu_i)\)</li>
<li>Para el caso Poisson \(Var(Y_i) = \mu_i\). </li>
<li>En estos últimos casos, a menudo es pertinente tener un modelo de varianza más flexible, incluso si no corresponde a una probabilidad real</li>
</ul>

<p>\[
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i (1 - \mu_i ) } W_i ~~~\mbox{y}~~~
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i} W_i
\]</p>

<ul>
<li>Estas son llamadas ecuaciones normales &#39;cuasi-verosímiles&#39; </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-95" style="background:;">
  <hgroup>
    <h2><em>Odds</em> y <em>ends</em></h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Las ecuaciones normales tienen que ser resueltas iterativamente, Resultando en \(\hat \beta_k\) y, Si está incluido, \(\hat \phi\).</p></li>
<li><p>Las respuestas predictoras predictoras lineales pueden obtenerse como \(\hat \eta = \sum_{k=1}^p X_k \hat \beta_k\)</p></li>
<li><p>Las respuestas medias predichas como \(\hat \mu = g^{-1}(\hat \eta)\)</p></li>
<li><p>Los coeficientes se interpretan como
\[
g(E[Y | X_k = x_k + 1, X_{\sim k} = x_{\sim k}]) - g(E[Y | X_k = x_k, X_{\sim k}=x_{\sim k}]) = \beta_k
\]
O el cambio en la función de enlace de la respuesta esperada por unidad de cambio en \(X_k\) manteniendo constantes otros regresores.</p></li>
<li><p>Las variaciones en el algoritmo de <em>Newton/Raphson</em> se utilizan para hacer esto.</p></li>
<li><p>Por lo general para hacer inferencia se utiliza la teoría asintotica</p></li>
<li><p>Muchas de las ideas de modelos lineales pueden ser vistas como MlGs.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-96" style="background:;">
  <hgroup>
    <h2>Respuestas Binarias</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Frecuentemente nos es de interés repuestas con solo dos posibles valores </p>

<ul>
<li>Sobrevive/No Sobrevive </li>
<li>Gana/pierde</li>
<li>Éxito/fracaso</li>
<li>etc.</li>
</ul></li>
<li><p>Llamadas Binarias, Bernoulli o salidas 0/1</p></li>
<li><p>La colección de resultados binarios intercambiables para los mismos datos de covariables se denomina binomial</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-97" style="background:;">
  <hgroup>
    <h2>Ejemplo</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">#download.file(&quot;https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda&quot;, destfile = &quot;./data/ravensData.rda&quot;, method = &quot;curl&quot;)
load(&quot;./data/ravensData.rda&quot;)
head(ravensData)
</code></pre>

<pre><code>##   ravenWinNum ravenWin ravenScore opponentScore
## 1           1        W         24             9
## 2           1        W         38            35
## 3           1        W         28            13
## 4           1        W         34            31
## 5           1        W         44            13
## 6           0        L         23            24
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-98" style="background:;">
  <hgroup>
    <h2>Regresión Lineal</h2>
  </hgroup>
  <article data-timings="">
    <p>\[ RW_i = b_0 + b_1 RS_i + e_i \]</p>

<p>\(RW_i\) - 1 Si el equipo gana, 0 si no </p>

<p>\(RS_i\) - Número de puntos marcados</p>

<p>\(b_0\) - probabilidad de que el equipo gane si marca 0 puntos</p>

<p>\(b_1\) - Incremento en la probabilidad de que el equipo gane por cada punto adicional</p>

<p>\(e_i\) - Variación residual debida</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-99" style="background:;">
  <hgroup>
    <h2>Regresión Lineal</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">lmRavens &lt;- lm(ravensData$ravenWinNum ~ ravensData$ravenScore)
summary(lmRavens)$coef
</code></pre>

<pre><code>##                         Estimate  Std. Error  t value   Pr(&gt;|t|)
## (Intercept)           0.28503172 0.256643165 1.110615 0.28135043
## ravensData$ravenScore 0.01589917 0.009058997 1.755069 0.09625261
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-100" style="background:;">
  <hgroup>
    <h2>Odds</h2>
  </hgroup>
  <article data-timings="">
    <p><strong>Salida Binaria 0/1</strong></p>

<p>\[RW_i\]  </p>

<p><strong>Probabilidad (0,1)</strong></p>

<p>\[\rm{Pr}(RW_i | RS_i, b_0, b_1 )\]</p>

<p><strong>Odds \((0,\infty)\)</strong></p>

<p>\[\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\] </p>

<p><strong>Log odds \((-\infty,\infty)\)</strong></p>

<p>\[\log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right)\] </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-101" style="background:;">
  <hgroup>
    <h2>Regresión logistica vs. Lineal</h2>
  </hgroup>
  <article data-timings="">
    <p><strong>Lineal</strong></p>

<p>\[ RW_i = b_0 + b_1 RS_i + e_i \]</p>

<p>o</p>

<p>\[ E[RW_i | RS_i, b_0, b_1] = b_0 + b_1 RS_i\]</p>

<p><strong>Logistica</strong></p>

<p>\[ \rm{Pr}(RW_i | RS_i, b_0, b_1) = \frac{\exp(b_0 + b_1 RS_i)}{1 + \exp(b_0 + b_1 RS_i)}\]</p>

<p>o</p>

<p>\[ \log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right) = b_0 + b_1 RS_i \]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-102" style="background:;">
  <hgroup>
    <h2>Interpretando la Regresión Logistica</h2>
  </hgroup>
  <article data-timings="">
    <p>\[ \log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right) = b_0 + b_1 RS_i \]</p>

<p>\(b_0\) - Logaritmo de los odds de una victoria del equipo si anota 0 puntos </p>

<p>\(b_1\) - Logaritmo de los odds probabilidad del proporción de victorias por cada punto anotado (comparado con cero puntos)</p>

<p>\(\exp(b_1)\) - Probabilidad de Proporción Odds por cada punto anotado (comparado con cero puntos)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-103" style="background:;">
  <hgroup>
    <h2><em>Odds</em></h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Imagine que está jugando un juego en el que tirar una moneda con probabilidad de éxito \(p\).</li>
<li>Si sale &quot;cara&quot;&quot;, gana \(X\). si sale &quot;sello&quot;, pierde \(Y\).</li>
<li><p>¿Como debemos establecer \(X\) y \(Y\) para que el juego sea justo?</p>

<p>\[E[ganancias]= X p - Y (1 - p) = 0\]</p></li>
<li><p>Implica 
\[\frac{Y}{X} = \frac{p}{1 - p}\]    </p></li>
<li><p>Los <em><em>odds</em></em> pueden decirse como &quot;¿Cuánto debería estar dispuesto a pagar por una probabilidad de \(p\) de ganar un dólar?&quot;</p>

<ul>
<li>(If \(p > 0.5\) Tiene que pagar más si pierde que lo que consigue si gana)</li>
<li>(If \(p < 0.5\) Tiene que pagar más si pierde que lo que consigue si gana)</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-104" style="background:;">
  <hgroup>
    <h2>Regresión logistica</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">logRegRavens &lt;- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,
                    family = &quot;binomial&quot;)
summary(logRegRavens)
</code></pre>

<pre><code>## 
## Call:
## glm(formula = ravensData$ravenWinNum ~ ravensData$ravenScore, 
##     family = &quot;binomial&quot;)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7575  -1.0999   0.5305   0.8060   1.4947  
## 
## Coefficients:
##                       Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)           -1.68001    1.55412  -1.081     0.28
## ravensData$ravenScore  0.10658    0.06674   1.597     0.11
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24.435  on 19  degrees of freedom
## Residual deviance: 20.895  on 18  degrees of freedom
## AIC: 24.895
## 
## Number of Fisher Scoring iterations: 5
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-105" style="background:;">
  <hgroup>
    <h2>Valores ajustados</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">plot(ravensData$ravenScore,
     logRegRavens$fitted, pch = 19,
     col = &quot;blue&quot; , xlab = &quot;Score&quot;, ylab = &quot;Prob Ravens ganen&quot;, frame.plot = F)
</code></pre>

<pre><code>## Error in plot(ravensData$ravenScore, logRegRavens$fitted, pch = 19, col = &quot;blue&quot;, : objeto &#39;ravensData&#39; no encontrado
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-106" style="background:;">
  <hgroup>
    <h2><em>Odds ratios</em> e intervalos de confianza</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">exp(logRegRavens$coeff)
</code></pre>

<pre><code>##           (Intercept) ravensData$ravenScore 
##             0.1863724             1.1124694
</code></pre>

<pre><code class="r">exp(confint(logRegRavens))
</code></pre>

<pre><code>## Waiting for profiling to be done...
</code></pre>

<pre><code>##                             2.5 %   97.5 %
## (Intercept)           0.005674966 3.106384
## ravensData$ravenScore 0.996229662 1.303304
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-107" style="background:;">
  <hgroup>
    <h2>ANOVA para la regresión logistica</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">anova(logRegRavens, test = &quot;Chisq&quot;)
</code></pre>

<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: ravensData$ravenWinNum
## 
## Terms added sequentially (first to last)
## 
## 
##                       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)  
## NULL                                     19     24.435           
## ravensData$ravenScore  1   3.5398        18     20.895  0.05991 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-108" style="background:;">
  <hgroup>
    <h2>Interpretando <em>Odds Ratios</em></h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>No probabilidades </li>
<li><em>Odds ratio</em> de 1 = no hay diferencia en los  <em>odds</em></li>
<li>log odds ratio de 0 = no hay diferencia en los  <em>odds</em></li>
<li>Odds ratio &lt; 0.5 o &gt; 2 Comúnmente un &quot;efecto moderado&quot;</li>
<li>Riesgo relativo \(\frac{\rm{Pr}(RW_i | RS_i = 10)}{\rm{Pr}(RW_i | RS_i = 0)}\), a menudo más fácil de interpretar, más difícil de estimar</li>
<li>Para probabilidades pequeñas RR \(\approx\) OR pero <strong>no son los mismo</strong>!</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-109" style="background:;">
  <hgroup>
    <h2>Idea principal - Regresión Poisson.</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Muchos eventos de interés toman la forma de conteos 

<ul>
<li>Número de llamadas a un Call Center</li>
<li>Número de casos de gripe en área determinada</li>
<li>Número de personas que cruzan un puente peatonal en un día </li>
</ul></li>
<li>Los datos también pueden tomar forma de proporciones 

<ul>
<li>Porcentajes de niños que pasan una prueba </li>
<li>Porcentaje de accesos a un sitio web de un país</li>
</ul></li>
<li>La regresión lineal con una transformación puede ser una opción. </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-110" style="background:;">
  <hgroup>
    <h2>Distribución Poisson</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>a distribución de Poisson es un modelo útil para conteos y tasas (proporciones)</li>
<li>Aquí una tasa es el conteo por cada tiempo de monitoreo</li>
<li>Algunos ejemplos de usos de la distribución de Poisson

<ul>
<li>Modelado del tráfico del acceso web</li>
<li>Tasas de accidentes</li>
<li>Aproximación de probabilidades binomiales con \(p\) pequeños y  \(n\) grandes</li>
<li>Analizar datos de tablas de contingencia</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-111" style="background:;">
  <hgroup>
    <h2>La función de densidad de la Poisson</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>\(X \sim Poisson(t\lambda)\) si</li>
</ul>

<p>\[
P(X = x) = \frac{(t\lambda)^x e^{-t\lambda}}{x!}
\]
Para \(x = 0, 1, \ldots\).</p>

<ul>
<li>La media de la  Poisson es \(E[X] = t\lambda\), esto es  \(E[X / t] = \lambda\)</li>
<li>La varianza de la Poisson \(Var(X) = t\lambda\).</li>
<li>La Poisson tiende a la normal cuando \(t\lambda\) se hace grande.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-112" style="background:;">
  <hgroup>
    <h2>Datos de un Website</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">download.file(&quot;https://dl.dropboxusercontent.com/u/7710864/data/gaData.rda&quot;,destfile=&quot;./data/gaData.rda&quot;,method=&quot;curl&quot;)
load(&quot;./data/gaData.rda&quot;)
gaData$julian &lt;- julian(gaData$date)
head(gaData)
</code></pre>

<pre><code>##         date visits simplystats julian
## 1 2011-01-01      0           0  14975
## 2 2011-01-02      0           0  14976
## 3 2011-01-03      0           0  14977
## 4 2011-01-04      0           0  14978
## 5 2011-01-05      0           0  14979
## 6 2011-01-06      0           0  14980
</code></pre>

<p><a href="http://skardhamar.github.com/rga/">http://skardhamar.github.com/rga/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-113" style="background:;">
  <hgroup>
    <h2>Gráfica de los datos</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">plot(gaData$julian, gaData$visits,pch = 19,
     col = &quot;darkgrey&quot;, xlab = &quot;Juliano&quot;, ylab = &quot;Visitas&quot;, frame.plot = &quot;F&quot;)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-20-1.png" title="plot of chunk unnamed-chunk-20" alt="plot of chunk unnamed-chunk-20" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-114" style="background:;">
  <hgroup>
    <h2>Regresión Lineal</h2>
  </hgroup>
  <article data-timings="">
    <p>\[ NH_i = b_0 + b_1 JD_i + e_i \]</p>

<p>\(NH_i\) - número de accesos (visitas) a la pagina </p>

<p>\(JD_i\) - día del año (día Juliano)</p>

<p>\(b_0\) - número de accesos en el día Juliano 0 (1970-01-01)</p>

<p>\(b_1\) - incremento en el número de visitas por unidad de día</p>

<p>\(e_i\) - Variación debida a todo lo que no se midió</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-115" style="background:;">
  <hgroup>
    <h2>Regresión Lineal</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">plot(gaData$julian,gaData$visits, pch = 19,
     col = &quot;darkgrey&quot;, xlab = &quot;Juliano&quot;, ylab = &quot;Visitas&quot;, frame.plot = &quot;F&quot;)
lm1 &lt;- lm(gaData$visits ~ gaData$julian)
abline(lm1, col = &quot;red&quot;, lwd = 3)
</code></pre>

<p><img src="assets/fig/linReg-1.png" title="plot of chunk linReg" alt="plot of chunk linReg" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-116" style="background:;">
  <hgroup>
    <h2>Aparte, tomando el logaritmo del resultado</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>tomar el logaritmo natural del resultado tiene una interpretación específica.</li>
<li>Considere el modelo</li>
</ul>

<p>\[ \log(NH_i) = b_0 + b_1 JD_i + e_i \]</p>

<p>\(NH_i\) - número de accesos (visitas) a la pagina </p>

<p>\(JD_i\) - día del año (día Juliano)</p>

<p>\(b_0\) - logaritmo del número de accesos en el día Juliano 0 (1970-01-01)</p>

<p>\(b_1\) - incremento en el logaritmo del número de visitas por unidad de día</p>

<p>\(e_i\) - Variación debida a todo lo que no se midió</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-117" style="background:;">
  <hgroup>
    <h2>Exponenciar los coeficientes</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>\(e^{E[\log(Y)]}\) media geométrica de \(Y\). 

<ul>
<li>Sin covariables, esto es estimado por \(e^{\frac{1}{n}\sum_{i=1}^n \log(y_i)} = (\prod_{i=1}^n y_i)^{1/n}\)</li>
</ul></li>
<li>Cuando se toma el logaritmo natural de los resultados y se ajusta un modelo de regresión, sus coeficientes exponenciales calculan las cosas sobre los medios geométricos.</li>
<li>\(e^{\beta_0}\) estima la media geométrica de visitas en el día 0</li>
<li>\(e^{\beta_1}\) estima el incremento o disminución relativa en media geométrica de las visitas a la pagina por día.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-118" style="background:;">
  <hgroup>
    <h2>Exponenciar los coeficientes</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Hay un problema con los logaritmos  cuando tenemos conteos de 0, la adición de una constante, puede funcionar </li>
</ul>

<pre><code class="r">round(exp(coef(lm(I(log(gaData$visits + 1)) ~ gaData$julian))), 5)
</code></pre>

<pre><code>##   (Intercept) gaData$julian 
##       0.00000       1.00231
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-119" style="background:;">
  <hgroup>
    <h2>Regresión Lineal vs. Poisson</h2>
  </hgroup>
  <article data-timings="">
    <p><strong>Lineal</strong></p>

<p>\[NH_i = b_0 + b_1 JD_i + e_i\]</p>

<p>o</p>

<p>\[E[NH_i | JD_i, b_0, b_1] = b_0 + b_1 JD_i\]</p>

<p><strong>Poisson/log-lineal</strong></p>

<p>\[\log\left(E[NH_i | JD_i, b_0, b_1]\right) = b_0 + b_1 JD_i\]</p>

<p>o</p>

<p>\[E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right)\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-120" style="background:;">
  <hgroup>
    <h2>Diferencias Multiplicativas</h2>
  </hgroup>
  <article data-timings="">
    <p><br><br>
\[ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right) \]</p>

<p><br><br></p>

<p>\[ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 \right)\exp\left(b_1 JD_i\right) \]</p>

<p><br><br></p>

<p>Si \(JD_i\) es el incremento por unidad, \(E[NH_i | JD_i, b_0, b_1]\) Se multiplica por \(\exp\left(b_1\right)\)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-121" style="background:;">
  <hgroup>
    <h2>Regresión Poisson en R</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">plot(gaData$julian,gaData$visits,pch = 19,
     col = &quot;darkgrey&quot;, xlab = &quot;Juliano&quot;, ylab = &quot;Visitas&quot;, frame.plot = &quot;F&quot;)
glm1 &lt;- glm(gaData$visits ~ gaData$julian, family = &quot;poisson&quot;)
abline(lm1, col = &quot;red&quot;, lwd = 3); 
lines(gaData$julian,glm1$fitted,col = &quot;blue&quot;,lwd = 3)
</code></pre>

<p><img src="assets/fig/poisReg-1.png" title="plot of chunk poisReg" alt="plot of chunk poisReg" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-122" style="background:;">
  <hgroup>
    <h2>Relación media-varianza ?</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">plot(glm1$fitted,glm1$residuals,pch = 19,
     col = &quot;grey&quot;, ylab = &quot;Residuals&quot;, xlab = &quot;Fitted&quot;, frame.plot = &quot;F&quot;)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-22-1.png" title="plot of chunk unnamed-chunk-22" alt="plot of chunk unnamed-chunk-22" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-123" style="background:;">
  <hgroup>
    <h2>Tasas</h2>
  </hgroup>
  <article data-timings="">
    <p><br><br></p>

<p>\[ E[NHSS_i | JD_i, b_0, b_1]/NH_i = \exp\left(b_0 + b_1 JD_i\right) \]</p>

<p><br><br></p>

<p>\[ \log\left(E[NHSS_i | JD_i, b_0, b_1]\right) - \log(NH_i)  =  b_0 + b_1 JD_i \]</p>

<p><br><br></p>

<p>\[ \log\left(E[NHSS_i | JD_i, b_0, b_1]\right) = \log(NH_i) + b_0 + b_1 JD_i \]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-124" style="background:;">
  <hgroup>
    <h2>Ajustando tasas en R</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">glm2 &lt;- glm(gaData$simplystats ~ julian(gaData$date),
            offset = log(visits + 1),
            family = &quot;poisson&quot;, data = gaData)
plot(julian(gaData$date), glm2$fitted, 
     col = &quot;blue&quot;, pch = 19, xlab = &quot;Fecha&quot;, ylab = &quot;Conteos  ajustados&quot;,
     frame.plot = &quot;F&quot;)
points(julian(gaData$date), glm1$fitted, col = &quot;red&quot;, pch = 19)
</code></pre>

<p><img src="assets/fig/ratesFit-1.png" title="plot of chunk ratesFit" alt="plot of chunk ratesFit" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-125" style="background:;">
  <hgroup>
    <h2>Ajustando tasas en R</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">glm2 &lt;- glm(gaData$simplystats ~ julian(gaData$date),
            offset = log(visits + 1),
            family = &quot;poisson&quot;, data = gaData)
plot(julian(gaData$date),gaData$simplystats/(gaData$visits + 1),
     col = &quot;grey&quot;, xlab = &quot;Fecha&quot;,
     ylab = &quot;Tasas ajustadas&quot;, pch = 19, frame.plot = &quot;F&quot;)
lines(julian(gaData$date),glm2$fitted/(gaData$visits + 1),
      col = &quot;blue&quot;, lwd = 3)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-23-1.png" title="plot of chunk unnamed-chunk-23" alt="plot of chunk unnamed-chunk-23" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-126" style="background:;">
  <hgroup>
    <h2>Árboles de decisión</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Divide iterativamente variables en grupos </li>
<li>Evalúa la &quot;homogeneidad&quot; dentro de los grupos </li>
<li>Divide nuevamente si es necesario </li>
</ul>

<p><strong>Pros</strong>:</p>

<ul>
<li>fácil de interpretar </li>
<li>Mejor desempeño en funciones no lineales </li>
</ul>

<p><strong>Cons</strong>:</p>

<ul>
<li>sin validación cruzada puede tender a al sobreajuste </li>
<li>hace mas difícil estimar la incertidumbre </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-127" style="background:;">
  <hgroup>
    <h2>Ejemplo</h2>
  </hgroup>
  <article data-timings="">
    <p><center><img src="assets/img/tree.png" alt=""></center></p>

<p><a href="http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg">http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-128" style="background:;">
  <hgroup>
    <h2>Algoritmo</h2>
  </hgroup>
  <article data-timings="">
    <ol>
<li>Comienza con todas las variables en un solo grupo </li>
<li>Encuentra la variable que mejor divide la salida </li>
<li>Divida los datos en dos grupos (&quot;hojas&quot;) y en esa división (&quot;nodo&quot;)</li>
<li>Con cada división, encuentra la variable que mejor divide la salida </li>
<li>Continúe hasta que los grupos sean demasiado pequeños o suficientemente &quot;puros&quot;</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-129" style="background:;">
  <hgroup>
    <h2>Medidas de &quot;Impureza&quot;</h2>
  </hgroup>
  <article data-timings="">
    <p>\[\hat{p}_{mk} = \frac{1}{N_m}\sum_{x_i\; in \; Leaf \; m}\mathbb{1}(y_i = k)\]</p>

<p><strong>Error de clasificación errónea</strong>: 
\[ 1 - \hat{p}_{m k(m)}; k(m) = {\rm most; common; k}\] </p>

<ul>
<li>0 = pureza perfecta</li>
<li>0.5 = sin pureza</li>
</ul>

<p><strong>Índice de Gini</strong>:
\[ \sum_{k \neq k'} \hat{p}_{mk} \times \hat{p}_{mk'} = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}) = 1 - \sum_{k=1}^K p_{mk}^2\]</p>

<ul>
<li>0 = pureza perfecta</li>
<li>0.5 = sin pureza</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-130" style="background:;">
  <hgroup>
    <h2>Medidas de &quot;Impureza&quot;</h2>
  </hgroup>
  <article data-timings="">
    <p><strong>Deviance/ganancia de información</strong>:</p>

<p>\[ -\sum_{k=1}^K \hat{p}_{mk} \log_2\hat{p}_{mk} \]</p>

<ul>
<li>0 = pureza perfecta</li>
<li>1 = sin pureza</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-131" style="background:;">
  <hgroup>
    <h2>Medidas de &quot;Impureza&quot;</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p><img src="assets/fig/leftplot-1.png" title="plot of chunk leftplot" alt="plot of chunk leftplot" style="display: block; margin: auto;" /></p>

<ul>
<li><strong>Calsificación erronea:</strong> \(1/16 = 0.06\)</li>
<li><strong>Gini:</strong> \(1 - [(1/16)^2 + (15/16)^2] = 0.12\)</li>
<li><strong>Inf:</strong> \(-[1/16 \times log2(1/16) + 15/16 \times log2(15/16)] = 0.34\)</li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <p><img src="assets/fig/unnamed-chunk-24-1.png" title="plot of chunk unnamed-chunk-24" alt="plot of chunk unnamed-chunk-24" style="display: block; margin: auto;" /></p>

<ul>
<li><strong>Calsificación erronea:</strong> \(8/16 = 0.5\)</li>
<li><strong>Gini:</strong> \(1 - [(8/16)^2 + (8/16)^2] = 0.5\)</li>
<li><strong>Inf:</strong> \(-[1/16 \times log2(1/16)+15/16 \times log2(15/16)] = 1\)</li>
</ul>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-132" style="background:;">
  <hgroup>
    <h2>Ejemplo: Iris Data</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">data(iris); suppressMessages(suppressWarnings(library(ggplot2)))
names(iris)
</code></pre>

<pre><code>## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot; 
## [5] &quot;Species&quot;
</code></pre>

<pre><code class="r">table(iris$Species)
</code></pre>

<pre><code>## 
##     setosa versicolor  virginica 
##         50         50         50
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-133" style="background:;">
  <hgroup>
    <h2>Conjuntos training y test</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">suppressMessages(suppressWarnings(library(caret)))
inTrain &lt;- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training &lt;- iris[inTrain,]
testing &lt;- iris[-inTrain,]
dim(training); dim(testing)
</code></pre>

<pre><code>## [1] 105   5
</code></pre>

<pre><code>## [1] 45  5
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-134" style="background:;">
  <hgroup>
    <h2>Iris petal widths/sepal width</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-25-1.png" title="plot of chunk unnamed-chunk-25" alt="plot of chunk unnamed-chunk-25" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-135" style="background:;">
  <hgroup>
    <h2>Iris petal widths/sepal width</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">modFit &lt;- train(Species ~ .,method = &quot;rpart&quot;,data = training)
print(modFit$finalModel)
</code></pre>

<pre><code>## n= 105 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 105 70 setosa (0.33333333 0.33333333 0.33333333)  
##   2) Petal.Length&lt; 2.5 35  0 setosa (1.00000000 0.00000000 0.00000000) *
##   3) Petal.Length&gt;=2.5 70 35 versicolor (0.00000000 0.50000000 0.50000000)  
##     6) Petal.Width&lt; 1.75 36  2 versicolor (0.00000000 0.94444444 0.05555556) *
##     7) Petal.Width&gt;=1.75 34  1 virginica (0.00000000 0.02941176 0.97058824) *
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-136" style="background:;">
  <hgroup>
    <h2>Gráfico tree</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">plot(modFit$finalModel, uniform = TRUE, 
      main = &quot;Classification Tree&quot;)
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .4)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-26-1.png" title="plot of chunk unnamed-chunk-26" alt="plot of chunk unnamed-chunk-26" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-137" style="background:;">
  <hgroup>
    <h2>Gráfico tree</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">suppressMessages(suppressWarnings(library(rattle)))
fancyRpartPlot(modFit$finalModel)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-27-1.png" title="plot of chunk unnamed-chunk-27" alt="plot of chunk unnamed-chunk-27" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-138" style="background:;">
  <hgroup>
    <h2>Predicciones</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">predict(modFit,newdata=testing)
</code></pre>

<pre><code>##  [1] setosa     setosa     setosa     setosa     setosa     setosa    
##  [7] setosa     setosa     setosa     setosa     setosa     setosa    
## [13] setosa     setosa     setosa     versicolor versicolor versicolor
## [19] versicolor versicolor versicolor versicolor versicolor versicolor
## [25] versicolor versicolor versicolor versicolor versicolor versicolor
## [31] virginica  versicolor virginica  virginica  virginica  virginica 
## [37] virginica  versicolor virginica  virginica  virginica  versicolor
## [43] virginica  virginica  virginica 
## Levels: setosa versicolor virginica
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-139" style="background:;">
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Contenido programático'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Contenido programático'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='Contenido programático'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Contenido programático'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Contenido programático'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Referencias'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Motivación'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Que es <em>Machine Learning</em>'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Ejemplo'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Formulación'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Que no es <em>Machine Learning</em>'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Un problema de  Regresión'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Predicción'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='<em>Statistical Learning</em>'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Por que estimar \(f\)'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Predicción'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='Predicción'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='Inferencia'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='Como estimamos \(f\)'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='Métodos Paramétricos'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='Métodos Paramétricos'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='Métodos No Paramétricos'>
         22
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=23 title='Compensación entre flexibilidad e interpretabilidad'>
         23
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=24 title='Componentes de un predictor'>
         24
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=25 title='Ejemplo SPAM'>
         25
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=26 title='Ejemplo SPAM'>
         26
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=27 title='Ejemplo SPAM'>
         27
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=28 title='Ejemplo SPAM'>
         28
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=29 title='Ejemplo SPAM'>
         29
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=30 title='Ejemplo SPAM'>
         30
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=31 title='Ejemplo SPAM'>
         31
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=32 title='Ejemplo SPAM'>
         32
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=33 title='Ejemplo SPAM'>
         33
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=34 title='Orden de la importancia relativa'>
         34
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=35 title='Basura entra = Basura sale'>
         35
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=36 title='Características'>
         36
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=37 title='Temas a considerar'>
         37
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=38 title='Compensación en predicción'>
         38
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=39 title='Error dentro vs Error fuera de la muestra'>
         39
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=40 title='Error dentro vs Error fuera de la muestra'>
         40
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=41 title='Regla de predicción 1'>
         41
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=42 title='Aplicando la regla 1 a <code>smallSpam</code>'>
         42
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=43 title='Regla de predicción 2'>
         43
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=44 title='Apliclando la Regla 2 a <code>smallSpam</code>'>
         44
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=45 title='Aplicando a la data completa <code>spam</code>'>
         45
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=46 title='Aplicando a la data completa <code>spam</code>'>
         46
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=47 title='La precisión'>
         47
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=48 title='Que sucede?'>
         48
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=49 title='Diseño de un estudio de predicción'>
         49
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=50 title='Diseño de un estudio de predicción'>
         50
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=51 title='Reglas básicas para el diseño'>
         51
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=52 title='Terminos básicos'>
         52
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=53 title='Cantidades clave'>
         53
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=54 title='Cantidades clave como fracciones'>
         54
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=55 title='Para datos continuos'>
         55
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=56 title='Medidas comunes de error'>
         56
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=57 title='Métodos de remuestreo'>
         57
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=58 title='Validación Cruzada - <em>(Cross Validation)</em>'>
         58
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=59 title='Validación Cruzada - <em>(Cross Validation)</em>'>
         59
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=60 title='Validación Cruzada - <em>(Cross Validation)</em>'>
         60
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=61 title='Conjunto de validación'>
         61
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=62 title='Conjunto de validación'>
         62
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=63 title='<em>Leave-One-Out Cross-Validation</em>'>
         63
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=64 title='<em>Leave-One-Out Cross-Validation</em>'>
         64
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=65 title='<em>k-Fold Cross-Validation</em>'>
         65
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=66 title='<em>Bootstrap</em>'>
         66
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=67 title='<em>Bootstrap</em>'>
         67
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=68 title='<em>Bootstrap</em>'>
         68
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=69 title='Regresión Lineal'>
         69
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=70 title='Verosimilitud'>
         70
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=71 title='Regresión Lineal'>
         71
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=72 title='Interpretando los coeficientes'>
         72
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=73 title='Interpretando los coeficientes'>
         73
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=74 title='Predicción'>
         74
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=75 title='Ejemplo'>
         75
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=76 title='Ejemplo'>
         76
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=77 title='Ajustando la regresión lineal'>
         77
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=78 title='\(\hat \beta_0\) Interpretable'>
         78
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=79 title='Cambio de escala'>
         79
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=80 title='Predicciones del precio de un diamante'>
         80
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=81 title='Regresión multivariada'>
         81
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=82 title='Estimaciones'>
         82
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=83 title='Las ecuaciones'>
         83
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=84 title='Ejemplo'>
         84
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=85 title='Interpretación de los coeficientes'>
         85
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=86 title='Valores ajustados, residuos y variación residual'>
         86
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=87 title='Modelos lineales'>
         87
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=88 title='Modelos lineales'>
         88
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=89 title='Modelos lineales generalizados'>
         89
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=90 title='Ejemplo, Modelos lineales'>
         90
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=91 title='Ejemplo, Regresión logistica'>
         91
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=92 title='Ejemplo, Regresión Poisson'>
         92
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=93 title='Detalles'>
         93
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=94 title='Acerca de las varianzas'>
         94
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=95 title='<em>Odds</em> y <em>ends</em>'>
         95
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=96 title='Respuestas Binarias'>
         96
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=97 title='Ejemplo'>
         97
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=98 title='Regresión Lineal'>
         98
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=99 title='Regresión Lineal'>
         99
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=100 title='Odds'>
         100
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=101 title='Regresión logistica vs. Lineal'>
         101
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=102 title='Interpretando la Regresión Logistica'>
         102
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=103 title='<em>Odds</em>'>
         103
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=104 title='Regresión logistica'>
         104
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=105 title='Valores ajustados'>
         105
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=106 title='<em>Odds ratios</em> e intervalos de confianza'>
         106
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=107 title='ANOVA para la regresión logistica'>
         107
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=108 title='Interpretando <em>Odds Ratios</em>'>
         108
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=109 title='Idea principal - Regresión Poisson.'>
         109
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=110 title='Distribución Poisson'>
         110
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=111 title='La función de densidad de la Poisson'>
         111
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=112 title='Datos de un Website'>
         112
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=113 title='Gráfica de los datos'>
         113
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=114 title='Regresión Lineal'>
         114
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=115 title='Regresión Lineal'>
         115
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=116 title='Aparte, tomando el logaritmo del resultado'>
         116
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=117 title='Exponenciar los coeficientes'>
         117
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=118 title='Exponenciar los coeficientes'>
         118
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=119 title='Regresión Lineal vs. Poisson'>
         119
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=120 title='Diferencias Multiplicativas'>
         120
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=121 title='Regresión Poisson en R'>
         121
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=122 title='Relación media-varianza ?'>
         122
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=123 title='Tasas'>
         123
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=124 title='Ajustando tasas en R'>
         124
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=125 title='Ajustando tasas en R'>
         125
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=126 title='Árboles de decisión'>
         126
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=127 title='Ejemplo'>
         127
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=128 title='Algoritmo'>
         128
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=129 title='Medidas de &quot;Impureza&quot;'>
         129
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=130 title='Medidas de &quot;Impureza&quot;'>
         130
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=131 title='Medidas de &quot;Impureza&quot;'>
         131
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=132 title='Ejemplo: Iris Data'>
         132
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=133 title='Conjuntos training y test'>
         133
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=134 title='Iris petal widths/sepal width'>
         134
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=135 title='Iris petal widths/sepal width'>
         135
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=136 title='Gráfico tree'>
         136
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=137 title='Gráfico tree'>
         137
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=138 title='Predicciones'>
         138
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=139 title='NA'>
         139
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  <script src="libraries/widgets/bootstrap/js/bootstrap.min.js"></script>
<script src="libraries/widgets/bootstrap/js/bootbox.min.js"></script>
<script src="libraries/widgets/quiz/js/jquery.quiz.js"></script>
<script src="libraries/widgets/quiz/js/mustache.min.js"></script>
<script src="libraries/widgets/quiz/js/quiz-app.js"></script>
<script src="libraries/widgets/interactive/js/ace/js/ace.js"></script>
<script src="libraries/widgets/interactive/js/opencpu-0.5.js"></script>
<script src="libraries/widgets/interactive/js/interactive.js"></script>

  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<script>  
  $(function (){ 
    $("#example").popover(); 
    $("[rel='tooltip']").tooltip(); 
  });  
  </script>  
  
  <script src="shared/shiny.js" type="text/javascript"></script>
  <script src="shared/slider/js/jquery.slider.min.js"></script>
  <script src="shared/bootstrap/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="shared/slider/css/jquery.slider.min.css"></link>
  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>